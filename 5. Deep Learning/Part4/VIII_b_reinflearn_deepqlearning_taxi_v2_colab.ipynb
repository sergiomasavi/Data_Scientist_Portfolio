{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tarjeta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Instalemos-las-librerias-en-Colab-para-tener-gym-y-poder-mostrar-los-videos-grabados\" data-toc-modified-id=\"1.-Instalemos-las-librerias-en-Colab-para-tener-gym-y-poder-mostrar-los-videos-grabados-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Instalemos las librerias en Colab para tener gym y poder mostrar los videos grabados</a></span></li><li><span><a href=\"#2.-Montemos-el-Agente-Deep-Q-Learning\" data-toc-modified-id=\"2.-Montemos-el-Agente-Deep-Q-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Montemos el Agente Deep Q-Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Empecemos-montando-una-función-para-entrenar-la-qtable\" data-toc-modified-id=\"Empecemos-montando-una-función-para-entrenar-la-qtable-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Empecemos montando una función para entrenar la qtable</a></span></li><li><span><a href=\"#Entrenamos-la-red-neuronal,-entrenar-los-pesos-del-agente-DQN\" data-toc-modified-id=\"Entrenamos-la-red-neuronal,-entrenar-los-pesos-del-agente-DQN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Entrenamos la red neuronal, entrenar los pesos del agente DQN</a></span></li><li><span><a href=\"#Montamos-la-función-de-evaluación-del-agente\" data-toc-modified-id=\"Montamos-la-función-de-evaluación-del-agente-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Montamos la función de evaluación del agente</a></span></li></ul></li><li><span><a href=\"#3.-Ejecutamos-el-main-del-proyecto\" data-toc-modified-id=\"3.-Ejecutamos-el-main-del-proyecto-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Ejecutamos el main del proyecto</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/isidreroyo/master-datascience-nucleus-academy/blob/master/reinforcement-learning/s08-reinflearn-qlearning/reinflearn_deepqlearning_taxi_v2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjr3vlWrBhOG"
   },
   "source": [
    "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnBoQcviBhOH"
   },
   "source": [
    "# Reinforced Learning con Q-learning - El taxi autónomo\n",
    "\n",
    "El primer ejemplo de reinforced learning más en serio que lo del multi-armed bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MIMXy_-BtxT"
   },
   "source": [
    "## 1. Instalemos las librerias en Colab para tener gym y poder mostrar los videos grabados\n",
    "Más detalles en este notebook de ejemplo del equipo de Colab https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H#scrollTo=7wY4qZhPXotR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wwE6DEvBl__"
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsgpKLMIBhOI"
   },
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v64O_k04CKqh"
   },
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfOcRdjJCNu-"
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnD6VzNRCTx8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v0Kre1RCZ-1"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w7Wfuj4epO1"
   },
   "source": [
    "Usaremos un nuevo tipo de almacenamiento diferente a los que habeis visto hasta ahora: diccionarios, arrays, listas... Hablo de *deque* que es básicamente una generalización de apilamienos y colas. Aunque podriamos usar una list, deque estan optimizadas para operaciones rápidas con tamaños fijos de datos, y gestiona mejor la memoria y los costes asociados a extraer datos del final dela cola (a la derecha) quitándolo al acabar la operacion, o insertar nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO6AsYz-fX8e"
   },
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYmBgBOzCkKf"
   },
   "source": [
    "Las dos siguientes funciones son para poder grabar en video los entornos de gym y mostrarlos\n",
    "\n",
    "Para activar el video, solo teneis que hacer \"env = wrap_env(env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO4yNfhLChi0"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def pintar_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Episodio: {frame['episodio']}\")\n",
    "        print(f\"Tiempo: {i + 1}\")\n",
    "        print(f\"Estado: {frame['state']}\")\n",
    "        print(f\"Accion: {frame['action']}\")\n",
    "        print(f\"Recompensa: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdXmbzKfDHoG"
   },
   "source": [
    "## 2. Montemos el Agente Deep Q-Learning\n",
    "\n",
    "En este caso montamos un conjunto de elementos incluido el Agente basado en redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INIAS4THf8di"
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    # Agente con red neuronal\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        self.obs_space = obs_space\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=15000)\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 32\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    # Metodo de \"recordar\" combinaciones de estados, acciones, recompensas, y estado siguiente\n",
    "    def remember(self, observation, action, reward, next_observation):\n",
    "        if len(self.memory) > self.memory.maxlen:\n",
    "            if np.random.random() < 0.5:\n",
    "                shuffle(self.memory)\n",
    "            self.memory.popleft()\n",
    "        self.memory.append((observation, action, reward, next_observation))\n",
    "\n",
    "    # Metodo para obtener el valor de q basado en un estado en concreto que se saca del predict de la red neuronal\n",
    "    def get_q(self, observation):\n",
    "        np_obs = np.reshape(observation, [-1, self.obs_space])\n",
    "        return self.model.predict(np_obs)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Montamos una red neuronal sencilla con una densidad basada en el número de acciones\n",
    "        # El tamaño de entrada es las observaciones, y la funcion activacion es lineal (sin sesgo!)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.action_space, input_shape=(self.obs_space,), activation='linear', use_bias=False))\n",
    "        model.compile(optimizer=Adam(), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_action(self):\n",
    "        # Como actualizamos la red con las nuevas observaciones y acciones\n",
    "        sample_transitions = random.sample(self.memory, self.batch_size)\n",
    "        random.shuffle(sample_transitions)\n",
    "        batch_observations = []\n",
    "        batch_targets = []\n",
    "\n",
    "        for old_observation, action, reward, observation in sample_transitions:\n",
    "            # Reshape targets to output dimension(=2)\n",
    "            targets = np.reshape(\n",
    "                self.get_q(old_observation), self.action_space)\n",
    "            targets[action] = reward  # Set Target Value\n",
    "            if observation is not None:\n",
    "                # If the old state is not a final state, also consider the\n",
    "                # discounted future reward\n",
    "                predictions = self.get_q(observation)\n",
    "                new_action = np.argmax(predictions)\n",
    "                targets[action] += self.gamma * predictions[0, new_action]\n",
    "\n",
    "            # Add Old State to observations batch\n",
    "            batch_observations.append(old_observation)\n",
    "            batch_targets.append(targets)  # Add target to targets batch\n",
    "\n",
    "        # Update the model using Observations and their corresponding Targets\n",
    "        np_obs = np.reshape(batch_observations, [-1, self.obs_space])\n",
    "        np_targets = np.reshape(batch_targets, [-1, self.action_space])\n",
    "        self.model.fit(np_obs, np_targets, epochs=1, verbose=0)\n",
    "\n",
    "    # Dos operadores para guardar y recuperar pesos de la red neuronal\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFpxRC4mkQzm"
   },
   "source": [
    "No aprendera desde cero el agente, se lo pondremos facil, ya que hemos visto que con QLearning podemos hacer muchas cosas sin mucho calculo. Así que montamos un agente basado en q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHqcYj06DHCS"
   },
   "outputs": [],
   "source": [
    "class TableAgent():\n",
    "    # Montamos un agente del estilo que hemos visto en anteriores ediciones\n",
    "    def __init__(self, actions, states):\n",
    "        self.actions = actions\n",
    "        self.states = states\n",
    "        # Fijaros que ponemos todos los valores a 150, en vez de empezar en 0\n",
    "        self.q_table = np.full((states, actions), 150) \n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9972\n",
    "        self.alpha = 0.8\n",
    "        self.gamma = 0.95\n",
    "\n",
    "    # Metodo de elección de la mejor acción basada en el estado, antes pasando\n",
    "    # por Epsilon-Gredy (al que le meteremos decay)\n",
    "    def select_action(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.actions)\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    # Metodo de actualizacion de los valores de la q-table como vimos\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.alpha * \\\n",
    "            (reward + (self.gamma * np.max(self.q_table[next_state])) - self.q_table[state][action]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOHJFvVBGa0-"
   },
   "source": [
    "### Empecemos montando una función para entrenar la qtable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwMYX--cDvlj"
   },
   "outputs": [],
   "source": [
    "def train_table():\n",
    "    # Entrenamos mediante q-learning y devolvemos la tabla qtable\n",
    "    env = gym.make('Taxi-v3')\n",
    "    agent = TableAgent(env.action_space.n, env.observation_space.n)\n",
    "\n",
    "    num_episodes = 10000\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    best_avg_reward = -math.inf\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        finepisodio = False        \n",
    "        while not finepisodio:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, finepisodio, _ = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, finepisodio)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            if finepisodio is True:\n",
    "                recent_rewards.append(episode_reward)\n",
    "\n",
    "        if (i_episode >= 100):\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            avg_rewards.append(avg_reward)\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "\n",
    "        # Montamos el Epsilon decay\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon = max(\n",
    "                agent.epsilon_min,\n",
    "                agent.epsilon *\n",
    "                agent.epsilon_decay)\n",
    "\n",
    "        print(\"\\rEpisodio {:05d}, epsilon = {:.3f}, mejor media = {:.3f}\".format(\n",
    "            i_episode, agent.epsilon, best_avg_reward), end=\"\")\n",
    "\n",
    "        if best_avg_reward >= 9.7:\n",
    "            print('\\nEntorno resuelto en {} episodios.'.format(i_episode), end=\"\")\n",
    "            break\n",
    "\n",
    "    return agent.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcTPKJFpHESy"
   },
   "source": [
    "Montamos una función para crear los pesos de la red neuronal a partir de la qtable aprendida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myIJE7y-HHll"
   },
   "outputs": [],
   "source": [
    "def create_agent_weights(q_table):\n",
    "    # Creamos el agente deep q-learning\n",
    "    agent = DQNAgent(500, 6)\n",
    "    \n",
    "    # Creamos los pesos (una tabla de 1, 500, 6)\n",
    "    weights = np.zeros((1, 500, 6))\n",
    "    best_actions = np.argmax(q_table, axis=1)\n",
    "    for n in range(0, 500):\n",
    "        weights[0][n][best_actions[n]] = 1\n",
    "\n",
    "    # Fijamos los pesos dentro de la red neuronal y los guardamos\n",
    "    agent.model.set_weights(weights)\n",
    "    agent.model.save_weights('taxi_model.h5')\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR8TtyBlINea"
   },
   "source": [
    "### Entrenamos la red neuronal, entrenar los pesos del agente DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mtXXvuRIsJ6"
   },
   "outputs": [],
   "source": [
    "def train_agent_weights(q_table):\n",
    "    # Esta función entrena el modelo del agente basado en elos datos de la Q-table\n",
    "    epochs = 800\n",
    "    batch_size = 16\n",
    "\n",
    "    # Construimos el modelo de Agente qtable\n",
    "    agent = DQNAgent(500, 6)\n",
    "\n",
    "    # Creamos los datos de entrenamiento (de lo aprendido en una q-table)\n",
    "    x_train = np.array([decode(i) for i in range(0,500)])\n",
    "    best_actions = np.argmax(q_table, axis=1)\n",
    "    y_train = np.zeros((500,6))\n",
    "    for n in range(500):\n",
    "        y_train[n][best_actions[n]]= 1\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    agent.model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Guardamos el modelo\n",
    "    agent.model.save_weights('taxi_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUlzHFYZQT89"
   },
   "source": [
    "### Montamos la función de evaluación del agente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U0l3Dg0QrmJ"
   },
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    env = gym.make(\"Taxi-v3\").env\n",
    "    action_space = env.action_space.n\n",
    "    observation_space = env.observation_space.n\n",
    "    agent = DQNAgent(observation_space, action_space)\n",
    "    agent.load(\"taxi_model.h5\")\n",
    "    scores = []\n",
    "\n",
    "    frames = [] # para la animación!\n",
    "\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        obs = decode(obs)\n",
    "        epochs, episode_reward = 0, 0\n",
    "        \n",
    "        findepisodio = False\n",
    "        \n",
    "        while not findepisodio:\n",
    "            q_values = agent.get_q(obs)\n",
    "            action = np.argmax(q_values)\n",
    "            obs, reward, findepisodio, _ = env.step(action)\n",
    "            obs_env = obs\n",
    "            obs = decode(obs)\n",
    "            episode_reward += reward\n",
    "            epochs += 1\n",
    "            # Ponemos cada frame renderizado dentro de un diccionatio para la animación\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'episodio': i,\n",
    "                'state': obs_env,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "            })\n",
    "\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "\n",
    "    # Presentamos los resultados del testeo\n",
    "    print(\"Puntuación: \", scores)\n",
    "    print(\"Media: \", np.mean(scores))\n",
    "\n",
    "    # Pintamos los scorings\n",
    "    plt.plot(scores)\n",
    "    plt.title('Fase de testeo')\n",
    "    plt.ylabel('Pasos de tiempo')\n",
    "    plt.ylim(ymax=10)\n",
    "    plt.xlabel('Prueba')\n",
    "    plt.savefig('TaxiAgentTesting.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Pintamos el testeo\n",
    "    pintar_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGVbtB9GQq25"
   },
   "source": [
    "Funciones adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "covi2BPToAE0"
   },
   "outputs": [],
   "source": [
    "def decode(i):\n",
    "    # Un array de longitud 500 se define como un one-hot vector\n",
    "    state = np.zeros(500)\n",
    "    state[i] = 1\n",
    "    return state\n",
    "\n",
    "\n",
    "def decode_to_18bit(i):\n",
    "    # Esta función decodifica el estado en un array de longitud 18, que contiene\n",
    "    # localizacion del taxi, localizacion pasajero y destino\n",
    "    out = []\n",
    "    out.append(i % 4)\n",
    "    i = i // 4\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i)\n",
    "    assert 0 <= i < 5\n",
    "    state = np.zeros(18)\n",
    "    # 5 bits for the taxi row\n",
    "    state[out[3]] = 1\n",
    "    # 5 bits for the taxi column\n",
    "    state[out[2] + 5] = 1\n",
    "    # 4 bits for the passenger\n",
    "    state[out[1] + 8] = 1\n",
    "    # 4 bits for the destination\n",
    "    state[out[0] + 12] = 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8euNv-wUsja"
   },
   "source": [
    "## 3. Ejecutamos el main del proyecto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zXJgounQrKD"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    q_table = train_table() # Entrenamos el agente q-learning\n",
    "    print(q_table)\n",
    "    create_agent_weights(q_table) # Fijamos los pesos de la NN del agente segun la q-table\n",
    "    #train_agent_weights(q_table) # Entrenamos los pesos de la NN del agente segun la q-table\n",
    "    test_agent() # Testeamos la red neuronal y vemos resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEy_Y7Qm_qkk"
   },
   "outputs": [],
   "source": [
    "train_agent_weights(q_table)\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP3LO33F_spp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VIII-b-reinflearn_deepqlearning_taxi_v2_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
