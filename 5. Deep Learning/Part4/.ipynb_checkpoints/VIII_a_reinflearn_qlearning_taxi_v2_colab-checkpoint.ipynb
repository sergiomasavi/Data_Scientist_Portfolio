{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tarjeta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Instalemos-las-librerias-en-Colab-para-tener-gym-y-poder-mostrar-los-videos-grabados\" data-toc-modified-id=\"1.-Instalemos-las-librerias-en-Colab-para-tener-gym-y-poder-mostrar-los-videos-grabados-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Instalemos las librerias en Colab para tener gym y poder mostrar los videos grabados</a></span></li><li><span><a href=\"#2.-A-por-el-entorno-del-Taxi\" data-toc-modified-id=\"2.-A-por-el-entorno-del-Taxi-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. A por el entorno del Taxi</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hagamos-memoria-del-problema\" data-toc-modified-id=\"Hagamos-memoria-del-problema-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Hagamos memoria del problema</a></span></li><li><span><a href=\"#La-tabla-de-recompensas\" data-toc-modified-id=\"La-tabla-de-recompensas-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>La tabla de recompensas</a></span></li></ul></li><li><span><a href=\"#3.-Que-pasa-si-lo-dejamos-hacer-solo-al-taxi-(no-Reinforcement-Learning)\" data-toc-modified-id=\"3.-Que-pasa-si-lo-dejamos-hacer-solo-al-taxi-(no-Reinforcement-Learning)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Que pasa si lo dejamos hacer solo al taxi (no Reinforcement Learning)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pintemos-los-resultados-en-una-bonita-animación\" data-toc-modified-id=\"Pintemos-los-resultados-en-una-bonita-animación-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pintemos los resultados en una bonita animación</a></span></li></ul></li><li><span><a href=\"#4.-El-turno-de-Q-Learning\" data-toc-modified-id=\"4.-El-turno-de-Q-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. El turno de Q-Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Inicializando-la-Q-table-a-una-matriz-de-500x6-llena-de-ceros\" data-toc-modified-id=\"4.1-Inicializando-la-Q-table-a-una-matriz-de-500x6-llena-de-ceros-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>4.1 Inicializando la Q-table a una matriz de 500x6 llena de ceros</a></span></li><li><span><a href=\"#4.2-Montemos-el-código-para-el-entrenamiento\" data-toc-modified-id=\"4.2-Montemos-el-código-para-el-entrenamiento-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>4.2 Montemos el código para el entrenamiento</a></span></li><li><span><a href=\"#4.3-Evaluemos-que-tal-lo-hace-nuestro-agente\" data-toc-modified-id=\"4.3-Evaluemos-que-tal-lo-hace-nuestro-agente-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>4.3 Evaluemos que tal lo hace nuestro agente</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnBoQcviBhOH"
   },
   "source": [
    "# Reinforced Learning con Q-learning - El taxi autónomo\n",
    "\n",
    "El primer ejemplo de reinforced learning más en serio que lo del multi-armed bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MIMXy_-BtxT"
   },
   "source": [
    "## 1. Instalemos las librerias en Colab para tener gym y poder mostrar los videos grabados\n",
    "Más detalles en este notebook de ejemplo del equipo de Colab https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H#scrollTo=7wY4qZhPXotR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wwE6DEvBl__"
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!apt-get install python-opengl -y\n",
    "!apt install xvfb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsgpKLMIBhOI"
   },
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v64O_k04CKqh"
   },
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay\n",
    "!pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfOcRdjJCNu-"
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnD6VzNRCTx8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v0Kre1RCZ-1"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYmBgBOzCkKf"
   },
   "source": [
    "Las dos siguientes funciones son para poder grabar en video los entornos de gym y mostrarlos\n",
    "\n",
    "Para activar el video, solo teneis que hacer \"env = wrap_env(env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO4yNfhLChi0"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdXmbzKfDHoG"
   },
   "source": [
    "## 2. A por el entorno del Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHqcYj06DHCS"
   },
   "outputs": [],
   "source": [
    "entorno = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "entorno.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM2kmvopDjUT"
   },
   "source": [
    "Esto se parece un poco a lo que hemos estado viendo en las slides, la única diferencia (que era así en la version v2 de Taxi) es que el taxi esta en la posición R, en vez de la (1,3) como teniamos pintado antes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBXsSKz7DyB3"
   },
   "source": [
    "El universo de la interfaz gym es <code> entorno </code>. Tenemos algunos métodos que iremos usando que os pueden ser útiles:\n",
    "\n",
    "<ul>\n",
    "  <li><code> entorno.reset </code> Pone el entorno en posición de fábrica, y nos devuelve un estado inicial random</li>\n",
    "  <li><code> entorno.step(action) </code> Paso del entorno tras un incremento de tiempo. Esto nos devuelve:\n",
    "  <ul>\n",
    "    <li><b>observation</b>: Observaciones del entorno</li>\n",
    "    <li><b>reward</b>: La recompensa que recogemos</li>\n",
    "    <li><b>done</b>: indica si hemos recogido o dejado un pasajero de forma exitosa, lo que denominaremos un <i>episodio</i></li>\n",
    "    <li><b>info</b>: Informacion adicional como la performance o la latencia de cara al debug</li>\n",
    "  </ul>\n",
    "  <li><code> entorno.render </code> Pinta un frame del entorno (muy útil para hacernos una idea del mismo)\n",
    "\n",
    "Nota adicional: hemos hecho <code>gym.make(\"Taxi-v3\").<b>env</b></code> para evitar que nos pare a las 200 iteraciones que es como funciona por defecto Gym\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOHJFvVBGa0-"
   },
   "source": [
    "### Hagamos memoria del problema\n",
    "\n",
    "Tenemos 4 localizaciones (con diferentes letras), y nuestro trabajo es recoger un pasajero en una de ellas y llevarlo a otra. Recibimos 20 puntos por una entrega exitosa, y perdemos 1 punto por cada paso de tiempo (para optimizar el recorrido). Hemos incorporado una penalización de 10 puntos por entregas o recogidas ilegales en localizaciones equivocadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwMYX--cDvlj"
   },
   "outputs": [],
   "source": [
    "entorno.reset()\n",
    "entorno.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcTPKJFpHESy"
   },
   "source": [
    "Fijaros que ahora el taxi esta en otra localización.\n",
    "\n",
    "Más cosas del entorno:\n",
    "*   La cajita amarilla es el taxi\n",
    "*   Las paredes o setos por donde no hay carretera son los pipes (|)\n",
    "*   R, G, Y, B son los puntos de recogida y entrega. El color <font color=\"blue\"><b>azul</b></font> indica punto de recogida, el color <font color=\"purple\"><b>purpura</b></font> indica punto de entrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myIJE7y-HHll"
   },
   "outputs": [],
   "source": [
    "print(\"Espacio acción {}\".format(entorno.action_space))\n",
    "print(\"Espacio estado {}\".format(entorno.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR8TtyBlINea"
   },
   "source": [
    "Fijaros que el Espacio Acción (action space) es de tamaño 6 y el Espacio Estado (state space) tiene tamaño 500. \n",
    "\n",
    "En terminos de mapeado que sepais que las acciones van de 0 a 5 con estos valores:\n",
    "\n",
    "*   0 = sur\n",
    "*   1 = norte\n",
    "*   2 = este\n",
    "*   3 = oeste\n",
    "*   4 = recoger pasajero\n",
    "*   5 = dejar pasajero\n",
    "\n",
    "Indice de puntos de recogida o entrega\n",
    "\n",
    "*   0 = R (0,0)\n",
    "*   1 = G (0,4)\n",
    "*   2 = Y (4,0)\n",
    "*   3 = B (4,3)\n",
    "\n",
    "Pero antes que nada, vamos a dar un vistazo a esa posición (3,1) que habiamos visto, veremos la información de ese estado. Con un pasajero esperando en la posición 2 (<font color=\"blue\"><b>Y</b></font>) con destino 0 (<font color=\"purple\"><b>R</b></font>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mtXXvuRIsJ6"
   },
   "outputs": [],
   "source": [
    "estado = entorno.encode(3, 1, 2, 0) # (fila taxi, columa taxi, indice posicion pasajero, indice posición destino)\n",
    "print(\"Estado:\", estado)\n",
    "\n",
    "entorno.s = estado # Podemos asignar el estado actual al que queramos definir\n",
    "entorno.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUlzHFYZQT89"
   },
   "source": [
    "### La tabla de recompensas\n",
    "\n",
    "Com podeis ver de las 0 a la 499 coordenadas que tenemos en nuestro entorno, seria interesante si lo que hemos dispuesto en nuestras recompensas en la presentación es lo mismo que hay en Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U0l3Dg0QrmJ"
   },
   "outputs": [],
   "source": [
    "entorno.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGVbtB9GQq25"
   },
   "source": [
    "Los índices del diccionario de la posición 328 son las acciones a tomar desde es punto. Y siguen una estructura muy clara:\n",
    "\n",
    "<code>{acción: [(probabilidad, siguiente estado, recompensa, realizado)]}</code>\n",
    "\n",
    "Como podéis ver, en este entorno hemos asignado una <code>probabilidad</code> a cada acción del 100% (no hacemos distinciones, ni forzamos un comportamiento del agente.\n",
    "\n",
    "<code>siguiente estado</code> nos indica en que coordenadas terminariamos si tomaramos la acción del índice.\n",
    "\n",
    "La <code>recompensa</code> se muestra en esa tercera posición, con los valores de -1 si \"añadimos\" un paso de tiempo más y -10 si al taxi se le ocurre recoger o dejar a un pasajero. Y si miraramos en la coordenada de destino correcta con un pasajero dentro del taxi, aparecería un bonito 20 en <code>recompensa</code> en la acción 5 (dejar pasajero).\n",
    "\n",
    "La posición <code>realizado</code> se usa para decirnos que hemos dejado un pasajero en la localización correcta. Cada entrega existosa sera nuestro final de **episodio**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8euNv-wUsja"
   },
   "source": [
    "## 3. Que pasa si lo dejamos hacer solo al taxi (no Reinforcement Learning)\n",
    "\n",
    "Por el método de la fuerza bruta. Se trata de usar la tabla <code>P</code> de recompensas, que nos será la guia para la navegación del taxi.\n",
    "\n",
    "La idea es montar un loop infinito que no se parará hasta que el taxi deje a un pasajero en su destino (un simple **episodio**). O cuando recibamos una recompensa de 20. \n",
    "\n",
    "El método <code>entorno.action_space.sample()</code> toma una acción de forma aleatoria de todas las posibles acciones.\n",
    "\n",
    "Veamos que ocurre...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zXJgounQrKD"
   },
   "outputs": [],
   "source": [
    "entorno.s = 328 \n",
    "\n",
    "epochs = 0\n",
    "castigo, recompensa = 0, 0\n",
    "\n",
    "total_epochs = 0\n",
    "total_castigos = 0\n",
    "\n",
    "frames = [] # para la animación!!\n",
    "\n",
    "finepisodio = False\n",
    "\n",
    "while not finepisodio:\n",
    "    accion = entorno.action_space.sample()\n",
    "    estado, recompensa, finepisodio, info = entorno.step(accion)\n",
    "\n",
    "    if recompensa == -10:\n",
    "        castigo += 1\n",
    "    \n",
    "    # Ponemos cada frame renderizado dentro de un diccionatio para la animación\n",
    "    frames.append({\n",
    "        'frame': entorno.render(mode='ansi'),\n",
    "        'episodio': epochs,\n",
    "        'state': estado,\n",
    "        'action': accion,\n",
    "        'reward': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "total_castigos += castigo\n",
    "total_epochs += epochs\n",
    "episodios = 1\n",
    "    \n",
    "print(\"Pasos de tiempo usados: {}\".format(epochs))\n",
    "print(\"Penalizaciones acumuladas: {}\".format(castigo))\n",
    "\n",
    "tiempomedio_random = total_epochs / episodios\n",
    "castigomedio_random = total_castigos / episodios\n",
    "\n",
    "print(f\"Resultados después de {episodios} episodios:\")\n",
    "print(f\"Media de tiempo por episodio: {tiempomedio_random}\")\n",
    "print(f\"Media de castigos por episodio: {castigomedio_random}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPT777mCWoeY"
   },
   "source": [
    "### Pintemos los resultados en una bonita animación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oupumuHzWsYT"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def pintar_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Episodio: {frame['episodio']}\")\n",
    "        print(f\"Tiempo: {i + 1}\")\n",
    "        print(f\"Estado: {frame['state']}\")\n",
    "        print(f\"Accion: {frame['action']}\")\n",
    "        print(f\"Recompensa: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "\n",
    "pintar_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SetUKvkIXzNH"
   },
   "source": [
    "Le ha costado la vida entregar al pasajero a su destino... no está mal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tzUDRkuZBTU"
   },
   "source": [
    "## 4. El turno de Q-Learning\n",
    "\n",
    "Recordemos que ahora hay que actualizar los valores de Q(a,s) en la Q-table, porque es de esa guía de donde sacaremos las mejores acciones para nuestro agente, el taxi.\n",
    "\n",
    "Valores más altos o mejores de Q indicaran para donde hemos de ir con nuestro taxi. Es decir, si el taxi se encuentra en un estado donde le espera un pasajero, es bastane probable que los valores para <code>recoger pasajero</code> sean mayores que el resto de acciones disponibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vmwiCWuaeUj"
   },
   "source": [
    "### 4.1 Inicializando la Q-table a una matriz de 500x6 llena de ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNCa-UvDY_ot"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([entorno.observation_space.n, entorno.action_space.n])\n",
    "\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rhFQY1ubVTo"
   },
   "source": [
    "Fijamos el tiempo, y nos cargamos unas cuantas librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxMCXXDkbLoQ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lh67D3xawhq"
   },
   "source": [
    "### 4.2 Montemos el código para el entrenamiento\n",
    "\n",
    "Definiremos los parámetros de actualización de la tabla Q (alpha, gamma) y el epsilon... ¡¡si nuestro amigo epsilon codicioso!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hefKWHPbbflY"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1 # Nuestro learning rate\n",
    "gamma = 0.6 # Nuestro descuento a la recompensa\n",
    "epsilon = 0.1 # Epsilon codicioso!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsqhS1KlbvCr"
   },
   "source": [
    "Definimos una variables para poder pintar metricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX06aUU2bzaJ"
   },
   "outputs": [],
   "source": [
    "all_epochs = []\n",
    "all_penalties = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2pAXSKab8Gy"
   },
   "source": [
    "Montamos el bucle de episodios, donde dentro meteremos el bucle del episodio en sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlg0bRlyb7gr"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 100001):\n",
    "  estado = entorno.reset() # Arrancamos el entorno en una posición aleatoria cada vez\n",
    "\n",
    "  epochs, castigo, recompensa = 0, 0, 0\n",
    " \n",
    "  finepisodio = False\n",
    "\n",
    "  while not finepisodio:\n",
    "    # Aquí va nuestro amigo Epsilon greedy, para fijar cuando explorarmos o explotamos\n",
    "    if np.random.random() < epsilon:\n",
    "      accion = entorno.action_space.sample() # Exploramos el espacio de acciones\n",
    "    else: \n",
    "      accion = np.argmax(q_table[estado]) # Explotamos lo aprendido\n",
    "    \n",
    "    siguiente_estado, recompensa, finepisodio, info = entorno.step(accion)\n",
    "\n",
    "    # Actualicemos los valores de la tabla Q en la posicion accion, estado tras ver lo que nos ha pasado\n",
    "    # Almacenamos el valor anterior de la tabla Q\n",
    "    valor_anterior = q_table[estado, accion]\n",
    "\n",
    "    # De la posicion estados, me quedo con el valor de la accion que daria el valor más alto en la tabla Q\n",
    "    siguiente_max = np.max(q_table[siguiente_estado])\n",
    "\n",
    "    # Calculamos la formulita de actualización de Q(a,s)\n",
    "    nuevo_valor = (1 - alpha) * valor_anterior + alpha * (recompensa + gamma * siguiente_max)\n",
    "    \n",
    "    q_table[estado, accion] = nuevo_valor\n",
    "\n",
    "    if recompensa == -10:\n",
    "      castigo += 1\n",
    "\n",
    "    estado = siguiente_estado\n",
    "    epochs += 1\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Episodio: {i}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entrenamiento terminado.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlygjkf-hL65"
   },
   "source": [
    "Una vez terminado el entrenamiento, veamos los valores de la posición 328 para cada acción disponible en ese entorno según lo aprendido por el taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92JnP0fqhKWN"
   },
   "outputs": [],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ4XPaW7tnsY"
   },
   "source": [
    "La acción 1 (ir al norte) es la que tiene mejor \"recompensa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBK0u5jFhfh9"
   },
   "source": [
    "### 4.3 Evaluemos que tal lo hace nuestro agente\n",
    "\n",
    "Ahora ya no exploramos más, así que la parte de Epsilon Greedy la quitamos y tiramos directamente de los valores de Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSwwJRU6htVJ"
   },
   "outputs": [],
   "source": [
    "total_epochs, total_castigos = 0, 0\n",
    "episodios = 100\n",
    "\n",
    "frames = [] # para la animación!!\n",
    "\n",
    "for i in range(episodios):\n",
    "    estado = entorno.reset()\n",
    "    epochs, castigo, recompensa = 0, 0, 0\n",
    "    \n",
    "    finepisodio = False\n",
    "    \n",
    "    while not finepisodio:\n",
    "        accion = np.argmax(q_table[estado])\n",
    "        estado, recompensa, finepisodio, info = entorno.step(accion)\n",
    "\n",
    "        if recompensa == -10:\n",
    "          castigo += 1\n",
    "        \n",
    "        epochs += 1\n",
    "         \n",
    "\n",
    "        # Ponemos cada frame renderizado dentro de un diccionatio para la animación\n",
    "        frames.append({\n",
    "          'frame': entorno.render(mode='ansi'),\n",
    "          'episodio': i,\n",
    "          'state': estado,\n",
    "          'action': accion,\n",
    "          'reward': recompensa\n",
    "          }\n",
    "        )\n",
    "\n",
    "    total_castigos += castigo\n",
    "    total_epochs += epochs\n",
    "\n",
    "\n",
    "\n",
    "tiempomedio_qlearning = total_epochs / episodios\n",
    "castigomedio_qlearning = total_castigos / episodios\n",
    "\n",
    "print(f\"Resultados después de {episodios} episodios:\")\n",
    "print(f\"Media de tiempo por episodio: {tiempomedio_qlearning}\")\n",
    "print(f\"Media de castigos por episodio: {castigomedio_qlearning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al5_VsPki5Qy"
   },
   "outputs": [],
   "source": [
    "pintar_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx4EYWOe8b_1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "VIII-a-reinflearn-qlearning-taxi-v2-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
