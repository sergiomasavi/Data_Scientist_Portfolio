{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tarjeta.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Generación-de-texto-mediante-redes-neuronales-recurrentes\" data-toc-modified-id=\"Generación-de-texto-mediante-redes-neuronales-recurrentes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Generación de texto mediante redes neuronales recurrentes</a></span></li><li><span><a href=\"#3.1-Indexado-de-carácteres\" data-toc-modified-id=\"3.1-Indexado-de-carácteres-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>3.1 Indexado de carácteres</a></span></li><li><span><a href=\"#3.2-Definimos-las-secuencias\" data-toc-modified-id=\"3.2-Definimos-las-secuencias-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3.2 Definimos las secuencias</a></span></li><li><span><a href=\"#3.3-Montamos-el-dataset\" data-toc-modified-id=\"3.3-Montamos-el-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3.3 Montamos el dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0iCRShPATnu"
   },
   "source": [
    "## Generación de texto mediante redes neuronales recurrentes\n",
    "\n",
    "Un ejemplo montado en Keras para RNN usando una GRU con un texto de Quijote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7vN5sT0ATnx"
   },
   "source": [
    "# 1. Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX-Tek_3wQIo"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from tensorflow import keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "!pip install unidecode\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uhDEuEMATn5"
   },
   "source": [
    "# 2. Cargamos los datos - el corpus basado en textos de Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr303fyeAToC"
   },
   "source": [
    "Preparamos los datos haciendo algunas manipulaciones, como la primera, decodificar el texto que viene en Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVdYwmmTCl6P"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "ruta = '/content/drive/MyDrive/Nuclio/materiales/datasets/quijote/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roehvv4BDpr4"
   },
   "source": [
    "Ejemplos del preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXMS67o5C7si"
   },
   "outputs": [],
   "source": [
    "print(unidecode.unidecode('Un amigo del Barça recogía un montón de castañas'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtYfNl4-0Pg6"
   },
   "outputs": [],
   "source": [
    "def preproceso(text):\n",
    "  # Sustituimos caratereces unicode por carateres ASCII\n",
    "  text = unidecode.unidecode(text)\n",
    "  # Reemplazamos saltos de linea e indicador de Byte Order Mark (BOM)\n",
    "  text = text.replace('\\n', ' ').replace('\\ufeff', '').lower()\n",
    "  # Filtramos caracteres\n",
    "  text = ''.join(x for x in text if x not in \"%&$#=<>/*+@][\")\n",
    "  # Reemplazamos dobles espacios por espacios simples\n",
    "  while len(text) != len(text.replace(\"  \", \" \")):\n",
    "    text = text.replace(\"  \", \" \")\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S6_W2CMW34e"
   },
   "outputs": [],
   "source": [
    "# Leemos el fichero y le aplicamos la función de preprocesado\n",
    "with open(ruta+\"quijote.txt\", 'r') as infile:\n",
    "  text = preproceso(infile.read())\n",
    "\n",
    "# A continuación vemos el tamaño del corpus (todo el texto del quijote)\n",
    "print ('Longitud del corpus: {} caracteres'.format(len(text)))\n",
    "print ('Ejemplo de texto...')\n",
    "print(text[:250])\n",
    "\n",
    "# Nos quedamos con los caracteres unicos para ver cuantos tenemos\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "print ('{} caracteres unicos'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x5h9dJcAToG"
   },
   "source": [
    "# 3. Pre-proceso de los datos\n",
    "\n",
    "## 3.1 Indexado de carácteres\n",
    "Montamos un indice para los carácteres, para tener valores numéricos, y vemos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rZMLuuUXTsQ"
   },
   "outputs": [],
   "source": [
    "# Creamos un diccionario donde las claves serán los caracteres y los valores sus indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# Creamos un numpy array con la lista de caracteres\n",
    "idx2char = np.array(vocab)\n",
    "# Convertimos el quijote a los indices de sus caracteres\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print(text_as_int[:20])\n",
    "print(text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da2gp8w3Xg1H"
   },
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "print ('{} ---- carácteres mapeados a números enteros ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vny-_e9AToR"
   },
   "source": [
    "Fijamos cada texto en secuencias de 100 carácteres, y mostramos como se va a ir entregando la inforamción a la red neuronal recurrente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKniYUAlXo4S"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "print(\"En cada epoch se procesarán:\",examples_per_epoch, \"frases\")\n",
    "\n",
    "# Convertimos estos textos a un formato que tensorflow entienda\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "  # Vemos que se ha creado con cada carácter un tensor, con el valor del indice que le hemos asignado\n",
    "  print(i, '->', idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEfUnFusAToW"
   },
   "source": [
    "## 3.2 Definimos las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybj51Zi0X3V-"
   },
   "outputs": [],
   "source": [
    "# Creamos batches de dimension de la secuencia + 1\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])), len(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDAb-M7AX8HF"
   },
   "outputs": [],
   "source": [
    "# Anteriormente hemos cogido usado el +1 para poder generar pares de (dato, predicción)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKw_n3b1YAeY"
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Secuencia de entrada: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Secuencia de salida:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfrKI_61YErq"
   },
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Paso {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  output esperado: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rime3ERQATor"
   },
   "source": [
    "## 3.3 Montamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUTxvrDtYKJ_"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "for item in dataset.take(2):\n",
    "  print(item[0].shape)\n",
    "  print(item[1].shape)\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISd67WS7AToz"
   },
   "source": [
    "# 4. Montamos la red neuronal recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW4s_v6FYuCv"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7qCpfNXY6yv"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = ks.Sequential()\n",
    "    model.add(ks.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    model.add(ks.layers.GRU(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True))\n",
    "    model.add(ks.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyttS9FZY90d"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bw-Lnu-ZItl"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ5x-oGoFFXg"
   },
   "source": [
    "# 5. Cogemos el modelo con pesos aleatorios que hemos creado y generamos una primera predicción con el mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsoEMUCuZA3c"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    print(input_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8O5ec4jZNU1"
   },
   "outputs": [],
   "source": [
    "# Cogemos una de las 64 frases que ha generado la prediccion\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "# Convertimos esta prediccion a un array de numpy\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUQ_LMQZZRbG"
   },
   "outputs": [],
   "source": [
    "# Enseñamos el array de numpy predicho (cada número corresponde a un carácter)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-VHjgdQZUYL"
   },
   "outputs": [],
   "source": [
    "# Observamos la entrada y la predicción que ha hecho nuestra red para cada carácter\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjCfTPPWF69y"
   },
   "source": [
    "# 6. Creamos una funcion de perdida loss\n",
    "\n",
    "La función de pérdida estándar <code>tf.keras.losses.sparse_categorical_crossentropy</code> funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
    "\n",
    "Debido a que nuestro modelo devuelve logits, necesitamos establecer la <code>from_logits</code>. Logits son un conjunto de probabilidades sin scalar, es decir, que no hay que meter ningún \"softmax\" en la salida de la red neuronal.\n",
    "\n",
    "Aprovechamos y calculamos el error escalar que estamos cometiendo en la predicción que hemos hecho en la anterior etapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLE08XdsZZSy"
   },
   "outputs": [],
   "source": [
    "# Aunque el problema no deja de ser de clasificación, ya que estamos\n",
    "# dando una palabra como predicción de entre todas las posibles, no nos interesa\n",
    "# que nos prediga una letra, queremos las probabilidades de todas\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Definimos una funcion de loss\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "# Probamos la función de loss\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN0gcaUGG1LZ"
   },
   "source": [
    "# 7. Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5axspBr2Zexm"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOugHVUEG7Db"
   },
   "source": [
    "# 8. Definimos checkpoints donde almacenar los modelos a cada epoch, usando callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7LsmraVZlCK"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=ks.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mc-Ny-qHFB0"
   },
   "source": [
    "# 9. Entrenamos el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alxAisExZoUP"
   },
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5jyuPm_QN9V"
   },
   "source": [
    "# 10. Reconstruimos el modelo con los pesos entrenados\n",
    "\n",
    "Debido a la forma en que el estado RNN se pasa de un paso de tiempo a otro, el modelo solo acepta un tamaño de batch fijo una vez construido.\n",
    "\n",
    "Para ejecutar el modelo con un batch_size diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chRUagh2ZxPC"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56_sp1guQ0uO"
   },
   "source": [
    "# 11.Montamos una función para generar texto\n",
    "\n",
    "Consiste en que iteremos generando 1000 caracteres a partir de una semilla definida.\n",
    "\n",
    "Existe un parametro llamado **temperatura** que lo modificaremos para ver los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWNak8zYZ_2-"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  text_generated = []\n",
    "  temperature = 0.5\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "      if idx2char[predicted_id] == \",\":\n",
    "        text_generated.append('\\n')\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnVdGwEyBf_i"
   },
   "source": [
    "# 12. Lanzamos nuestra predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIqNtAUUREpb"
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"dulcinea \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czbjsa_Ti4oi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VI-a-rnn_textgeneration_keras_gru_quijote.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
